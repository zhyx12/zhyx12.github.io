<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">






  <link rel="canonical" href="http://yoursite.com/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Hexo</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Startseite</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archiv</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/11/pytorch-0.4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/11/pytorch-0.4/" itemprop="url">
                  Pytorch 0.4迁移指南
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Post created: 2018-09-11 19:11:46 / Updated at: 19:16:00" itemprop="dateCreated datePublished" datetime="2018-09-11T19:11:46+08:00">2018-09-11</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="版本迁移">版本迁移</h1>
<p>0.4最大的改变在于将Tensor和Variable合并。本文的目的在于厘清一些之前比较模糊的概念，如求导机制中的requires_grad，volatile，leaf，non-leaf等，并介绍新旧版本的差异。</p>
<h1 id="自动求导">自动求导</h1>
<p>requires_grad是自动求导的核心标志,可以通过在每一次前向运行网络时设置requires_grad，来控制loss.backward()时部分网络参数的梯度为0。</p>
<p>网络权重的梯度的累加和optimizer中是否有该权重是相互独立的。</p>
<p>optimizer只需要创建的时候参数的requires_grad=True就行，step()时=False也没有关系</p>
<h2 id="旧版本">旧版本</h2>
<p>之前版本 Variable的属性：</p>
<ul>
<li>data</li>
<li>grad</li>
<li>requires_grad</li>
<li>volatile</li>
<li>data</li>
<li>grad</li>
</ul>
<h3 id="和optimizer的关系">和optimizer的关系</h3>
<ol style="list-style-type: decimal">
<li>错误信息：optimizing a parameter that doesn't require gradients。送到优化器中的参数需要requires_grad = True</li>
<li>错误信息：can't optimize a non-leaf Variable 送到优化器中的参数只能是leaf或者网络的权重参数，不能是中间的输出变量，如特征图（特征图一般也不需要优化，只需要传递误差就行）</li>
</ol>
<p>leaf指的是用户自己定义的变量，其他的正常情况下都是non-leaf（使用了detach_()方法也会变成leaf）。leaf和non-leaf的概念是在计算图的每一步的结果上定义的，CNN的权重没有leaf和non-leaf的概念。但是权重有requires_grad属性</p>
<p>loss.backward()计算得到的梯度值，可以通过.grad属性得到</p>
<p>对于中间变量（non-leaf Variable）, 可以通过retain_grad()来使得保存变量的梯度值</p>
<p>detach() 函数返回一个新的Variable，从当前的计算图中脱离，不需要计算梯度。返回的Variable和原来的Variable共用一个tensor数据，in-place的操作会彼此影响。</p>
<p>register_hook(hook)注册一个反向传播的hook,hook是一个函数，输入是正常的grad,输出是新的grad或者None hook(grad) -&gt; Tensor or None</p>
<p>detach_() 从计算图中脱离，成为一个叶子节点。detach_()之后required_grad自动变成False</p>
<h2 id="新版本">新版本</h2>
<p>Variable的属性 - data 保留，不变 - grad 保留 - requires_grad 保留 - volatile 废弃 - is_leaf 保留 - grad_fn 保留</p>
<p>volatile去掉了，改成了使用上下文管理器来设置是否需要计算grad</p>
<h3 id="data属性和detech">.data属性和detech()</h3>
<p>在旧版本里.data属性获取的是tensor，就和计算图没有关系了，同时共享了底层的数据，如果对tensor进行in-place的操作，也会对梯度的反向传播产生影响（该变量之前的变量的梯度计算会出错），但是不会报错。 0.4版本中.data属性保持了同样的含义，会返回一个共享数据的Tensor,和计算历史无关，同时requires_grad = False，如果进行了in-place的操作，效果同上。</p>
<p>推荐的方式是使用detach()，返回一个新的Variable或者tensor。如果detach()之后的量进行了in-place操作，会报错，但是抱错的位置不一样。0.3.1更早一点。</p>
<p>0.3.1版本中 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3.</span>], requires_grad = <span class="keyword">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = a.sigmoid()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = out.detach()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.zero_()</span><br><span class="line">tensor([ <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line">这里就会报错</span><br><span class="line">RuntimeError: <span class="keyword">in</span>-place operations can be only used on variables that don<span class="string">'t share storage with any other variables, but detected that there are 2 objects sharing it</span></span><br></pre></td></tr></table></figure></p>
<p>0.4.0版本 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.tensor([1,2,3.], requires_grad = True)</span><br><span class="line">&gt;&gt;&gt; out = a.sigmoid()</span><br><span class="line">&gt;&gt;&gt; c = out.detach()</span><br><span class="line">&gt;&gt;&gt; c.zero_()</span><br><span class="line">tensor([ 0., 0., 0.])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; out # modified by c.zero_() !!</span><br><span class="line">tensor([ 0., 0., 0.])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; out.sum().backward() # Requires the original value of out, but that was overwritten by c.zero_()</span><br><span class="line">RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation</span><br></pre></td></tr></table></figure></p>
<h3 id="loss累加">loss累加</h3>
<p>之前版本中常用的计算多个batch的平均loss，用于输出的代码 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_loss += loss.data[0]</span><br></pre></td></tr></table></figure></p>
<p>现在由于 loss是一个scalar，维度是0，所以[0]是会出错的，应该改成 loss.item() <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_loss += loss.item()</span><br></pre></td></tr></table></figure></p>
<p><strong>Note:</strong> item()方法只针对包含一个元素的tensor适用</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/05/distribution-adaptation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/05/distribution-adaptation/" itemprop="url">
                  Distribution Adaptation
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Post created: 2018-09-05 17:14:45 / Updated at: 17:14:56" itemprop="dateCreated datePublished" datetime="2018-09-05T17:14:45+08:00">2018-09-05</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="方法">方法</h1>
<p>Distributing Adaptation是数据分布自适应，这里主要介绍DA中的数据分布自适用的浅层方法。</p>
<p>分为边缘分布自适应，条件分布自适应和联合分布自适应。其中边缘分布自适应是最早也是最基础的方法。</p>
<p>TCA是经典的边缘分布自适应方法，使用了MMD准则作为领域（分布）之间距离的度量方法。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/05/DA-in-semantic-segmentation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/05/DA-in-semantic-segmentation/" itemprop="url">
                  DA in Semantic Segmentation
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Post created: 2018-09-05 17:14:11" itemprop="dateCreated datePublished" datetime="2018-09-05T17:14:11+08:00">2018-09-05</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Updated at: 2018-09-17 22:24:25" itemprop="dateModified" datetime="2018-09-17T22:24:25+08:00">2018-09-17</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="数据集">数据集</h1>
<h2 id="cityscapes">Cityscapes</h2>
<p>34个类别 图像分辨率2048x1024，训练集2975，验证集500，测试集1525 数据集的划分是城市级别的，训练集18个城市，验证集3个，测试集6个，不交叉</p>
<h2 id="synthia">SYNTHIA</h2>
<p>有13个类别 对于season-&gt;season任务，使用SYNTHIA-VIDEO-SEQUENCES。包括7段视频，涵盖不同的场景（），下面有不同的子设置：季节，天气，光照。图像有8个RGB相机拍摄构成360度的视角。为了减小视角的影响，只选择和行车记录仪视角类型的图像。</p>
<p>对于synthetic -&gt; real任务，使用SYNTHIA-RAND-CITYSCAPES。从所有的视屏中随机选取了9000张图像，和cityscapes的标签兼容。</p>
<h2 id="gta5">GTA5</h2>
<p>从Grand Theft Auto V的开放世界中获取的包含24996张高质量图像。图像分辨率1914x1052，城市是基于Los Angeles的Los Santos。</p>
<p>BDDS</p>
<h1 id="论文">论文</h1>
<p>全卷积网络在密集预测中被认为是有效的，但是遇到领域偏移的时候效果会下降。一些工作利用弱标签来提升语义分割的效果。<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>中使用带注意力机制的编码解码结构来迁移弱的类别标签，<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>中迁移物体的位置信息。</p>
<p>更多的工作关注语义分割中的深度无监督域适应。</p>
<h1 id="fcns-in-the-wild">FCNs in the wild</h1>
<p><a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> 首先引入了这一概念，在FCN上进行对抗式的训练以实现整个领域的对齐。迁移空间布局是利用类别相关的constrained multiple instance loss。<a href="https://github.com/Wanger-SJTU/FCN-in-the-wild" target="_blank" rel="external">代码，pytorch</a></p>
<p>作者指出在语义分割中有两个主要的领域偏移。首先是全局的变化会导致特征空间的边缘分布不同，这一差异会发生在任何两个不同的领域，但是在差异比较大的领域之间占主要部分；其次是特定类别之间的差异，比如不同城市之间车辆以及交通信号的偏移。作者因此采用了两个loss来分别处理。</p>
<p>全局的loss是保证特征空间表示的一致性，是不是应在靠后一点？ dense（有空间信息）的特征一致性和一维向量（分类任务）的特征一致性有什么区别吗？ 最后的dense的特征，每一个空间位置的一维向量都可以看做是分类任务的特征，不同位置之间的关系是不是可以利用一下？</p>
<p>Hoffman et al. [13] introduce the task of domain adaptation on semantic seg- mentation by applying adversarial learning (i.e., DANN) in a fully-convolutional way on feature representations and additional category constraints similar to the constrained CNN [29]. ----<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> category specific adaptation是从弱语义分割学习的方法中借鉴过来的。还需要看一下 Constrained convolutional neural networks for weakly supervised segmentation. Fully convolutional multi-class multiple instance learning.</p>
<h2 id="指标">指标</h2>
<p>数据集：Cityscapes， SYNTHIA，GTA5，BDDS(proposed) 分了三类任务： cities-&gt;cities，season-&gt;season，synthetic-&gt;real 其中cities-&gt;cities，synthetic-&gt;real任务使用的是cityscapes的19类标签 season-&gt;season使用的是SYNTHIA的13类标签</p>
<p>GAT5 -&gt; Cityscapes： before adaptation 21.1 after adaptation 27.1</p>
<p>SYNTHIA Cityscpes： 14.7 17.0</p>
<h2 id="实验">实验</h2>
<h3 id="对抗训练">对抗训练</h3>
<p>对抗训练是在有空间信息的特征图上，对每一个空间位置都进行源域vs目标域的分类，而不是最后输出1x1的特征图输出然后判断。</p>
<p>和learning to adapt相比，在反向训练（<span class="math inline">\(L_{inv}\)</span>）的时候，使用了源域的数据，而learning to adapt没有。对应公式(4)</p>
<p>这里是迭代训练的，而leaning to adapt是反向传播loss并叠加，然后一起更新。</p>
<p>这里在反向训练（<span class="math inline">\(L_{inv}\)</span>）的时候，也用了正向训练的损失（其实和learning to adapt的一次更新过程一样）。对应公式(7) ### 类别的适应 对于特定类别，通过每一张图片都可以得到一个比例，统计所有图片，可以得到一个直方图 <span class="math inline">\(\alpha_c\)</span>是下10%的界，<span class="math inline">\(\delta_c\)</span>是均值，<span class="math inline">\(\gamma_c\)</span>是上10%的界</p>
<p>对目标图像使用像素比例上的限制。但是为什么要用图像级别的标签呢？</p>
<h1 id="curriculum-domain-adaptationcda">Curriculum domain adaptation(CDA)</h1>
<p><a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> ICCV2017 使用虚拟图像来增强现实图像的语义分割效果，使用了图像级别的全局标签分布损失，和超像素级别的局部标签分布损失来正则化模型的微调过程。<a href="https://github.com/YangZhang4065/AdaptationSeg" target="_blank" rel="external">代码，keras</a> 数据集：Cityscapes， SYNTHIA，GTA5</p>
<p>课程学习，更倾向于学习一个数据的分布。</p>
<p>颜色的一致性在由虚拟到真实的迁移也很重要。使用了现成的方法。</p>
<p>能不能像STN那样有一个显式的转换模块</p>
<p><a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a><a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a><a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a><a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a><a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a>是之前或同一时期的工作。VisDA2017中获胜的方案是很多模型的集成。</p>
<p>softmax输出的概率层面的分布的匹配和特征空间的概率匹配。</p>
<p>Constrained convolutional neural networks for weakly supervised segmentation. ICCV2015 Training constrained deconvolutional networks for road scene semantic segmentation.</p>
<h1 id="no-more-discrimination">No more discrimination</h1>
<p><a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a> ICCV2017 中跨城市语义分割的框架，为目标域的像素赋予伪标签，然后通过领域对抗学习实现全局和类别的对齐。<a href="">代码</a> <a href="https://yihsinchen.github.io/segmentation_adaptation/" target="_blank" rel="external">项目主页</a> 作者考虑了两类领域偏移的情况，一类是全局的偏移，一类是类别的偏移。前者是由于城市外观的差异，后者是由于道路上不同的组成情况。</p>
<p>全局偏移的loss定义和FCN in the wilds是一样的，也是迭代训练，但是正向的损失函数没有参与到反向的迭代过程中。全局的loss也是分开grid来做的。</p>
<p>类别的适应在Fcn in the wild也有，但是那里是利用类别的组成比例，即假设不同城市之间的组成是相似的。本文拓展了JDA的思想，为目标域的图像赋予伪标签，在全局的域适应完成之后，目标域的图像可以产生语义预测结果。因此，源域和目标域之间的类别关联就可以获得了。类别相关的对抗是在每一个grid的每一类别上进行的，没有对同一位置的类别进行max之后再构造对抗损失。为每一个grid计算类别概率均值，然后对于每一类别在图像空间内进行平均。这一平均之后的值作为对抗损失的系数。</p>
<p>利用Google Street View构建了一个数据集</p>
<h2 id="相关论文">相关论文</h2>
<p>数据偏差论文： A. Khosla, T. Zhou, T. Malisiewicz, A. A. Efros, and A. Tor- ralba. Undoing the damage of dataset bias. In ECCV. Springer, 2012. A. Torralba and A. A. Efros. Unbiased look at dataset bias. In CVPR. IEEE, 2011</p>
<p>网络结构如下图所示 </p>
<p>和Learning to adapt structured output space的作者是一样的。</p>
<h2 id="指标-1">指标</h2>
<p>SYNTHIA - Cityscapes 预训练 30.7 全局 33.8 全局+类别 35.7</p>
<h1 id="road-net">ROAD-Net</h1>
<p><a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a> CVPR2018 使用了目标引导的蒸馏模型来迁移真实图像的风格，同时使用了空间感知适应模块来利用内在的空间结构来减小领域偏移。不同于在特征空间上直接进行简单的对抗损失。<a href="http://www.vision.ee.ethz.ch/~yuhchen/" target="_blank" rel="external">作者主页</a></p>
<p>从特征表示的角度讲，模型会在虚拟数据集上过拟合，使得在真实图像上不适合。从分布的角度讲，虚拟数据和真实数据的分布不匹配。</p>
<p>首先使用目标域引导的蒸馏方法来学习真实图像的风格，通过让分割模型学习在真实图片上预训练好的模型。然后，提出了空间感知模块来对齐两个领域分布。这两个模型可以添加到任意的分割模型中。</p>
<p>受到模型蒸馏论文的启发，使用了在ImageNet上预训练好的模型（参数固定）来充当老师，在虚拟数据集上训练时构造一个蒸馏损失（特征的欧式距离），使模型输出的特征不要过分地偏向于虚拟数据。还有其他的方法防止在虚拟数据集上的过拟合，比如固定某些层，或者使用learning without forgeting的方法。</p>
<p>空间感知的适应模块是将图像划分成不同的块，然后在每一块上使用领域对抗损失</p>
<p>相关工作：<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a> <a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a><a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a><a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a><a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a><a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a></p>
<h2 id="问题">问题</h2>
<p>However, aligning pixel-level features between synthetic and real data for urban scene images is non-trivial. Conventional domain adaptation approaches are usually proposed for the image classification task. While similar methodol- ogy can be applied by taking each pixel-level feature as a training sample, it is still challenging to fully reduce the distribution mismatch, since the pixels vary significantly in appearance [17] and scale [2]. 挖掘局部特征之间的关联？或者进行max之类的操作然后再对齐。</p>
<p>Hoffman et al. [17] implemented it by switching domain label similarly as in the Generative Adversarial Networks (GAN) [11]. We follow [8] to insert a gradient reverse layer between F and h. Particularly,</p>
<h2 id="指标-2">指标</h2>
<p>数据集：GTA5→Cityscapes DeepLab: 35.9 PSPNet: 39.4</p>
<h1 id="learning-from-synthetic-data">Learning From Synthetic Data</h1>
<p><a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a> CVPR2018 使用了GAN将特征映射到图像空间（新的图像空间和原图像可以使用reconstruction loss），在图像空间使用判别器。<a href="https://github.com/swamiviv/LSD-seg" target="_blank" rel="external">代码，pytorch</a></p>
<p>相关论文：<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a><a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a></p>
<p>网络结构： <img src="/2018/09/05/DA-in-semantic-segmentation/learning-from-synthetic-data-framework.JPG" title="learning-from-synthetic-data网络结构"></p>
<p>Unsupervised Image-to-Image Translation Networks(UNIT)的网络结构如下图所示 <img src="/2018/09/05/DA-in-semantic-segmentation/UNIT-network-arch.JPG" title="UNIT网络结果"></p>
<p>Since the advent of deep neural networks, emphasis has been shifted to learning do- main invariant features in an end-to-end fashion. Astandard framework for deep domain adaptation involves minimizing a measure of domain discrepancy along with the task being solved. Some approaches use Maximum Mean Discrepancy and its kernel variants for this task ( [21], [22]), while oth- ers use adversarial approaches ( [7], [2], [28]).</p>
<h2 id="指标-3">指标</h2>
<p>数据集： SYNTHIA→CITYSCAPES FCN8s-VGG16 36.1</p>
<p>GTAV→CITYSCAPES FCN8s-VGG16 37.1</p>
<h1 id="learning-to-adapt-structured-output-space">Learning to adapt structured output space</h1>
<p><a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a> CVPR2018，<a href="https://github.com/wasidennis/AdaptSegNet" target="_blank" rel="external">代码,pytorch0.4</a></p>
<p>考虑到语义分割是结构化的输出，包含了源域和目标域之间的相似性，在输出空间使用对抗学习。为了进一步增强适应模型，我们在不同的特征层的输出上进行对抗式域适应。</p>
<p>首先是在输出的语义图（是在<span class="math inline">\(c\times h\times w\)</span>的特征图上，没有经过max操作）上进行对抗训练，也是structured的来源。其次，在多个特征图上进行，如conv5和conv4，进行对抗训练，并且使用这两层的输出构造语义分割损失。</p>
<p>对抗损失的任务本身是一个分类任务，但是和前面一样，都是在特征图的不同位置上都进行对抗损失的计算。</p>
<p>尽管论文中这样写 Given the segmentation softmax output <span class="math inline">\(P = G(I) ∈ R^H×W×C\)</span>, where C is the number of categories, we forward P to a fully-convolutional discriminator D using a cross-entropy loss <span class="math inline">\(L_d\)</span> for the two classes (i.e., source and target). 但是送到判别的输入不是softmax之后的值，而是softmax前面一步的值，虽然二者的大小关系是一致的。</p>
<p>网络结构如下图  数据集： GTA5→Cityscapes， VGG16 35.0 ResNet101 42.4 SYNTHIA→Cityscapes VGG16 37.6 Resnet101 46.7</p>
<h2 id="相关工作">相关工作</h2>
<p><a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a> introduce the task of domain adaptation on semantic seg- mentation by applying adversarial learning (i.e., DANN) in a fully-convolutional way on feature representations and additional category constraints similar to the constrained CNN[29]. <a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a> focus on adapting synthetic-to- real or cross-city images by adopting class-wise adversarial learning [3] or label transfer [3]. <a href="#fn26" class="footnoteRef" id="fnref26"><sup>26</sup></a> Similar to the PixelDA method [1], one concurrent work, CyCADA [12] uses the CycleGAN [42] and transfers source domain images to the target domain with pixel alignment, thus generating extra training data combined with feature space adversarial learn- ing [13].</p>
<p>Recently, the PixelDA method [1] addresses domain adap- tation for image classification by transferring the source images to target domain, thereby obtaining a simulated train- ing set for target images.</p>
<h2 id="指标-4">指标</h2>
<p>实验结果中的oracle指的是在目标域数据集的训练集上训练，测试集上测试的结果</p>
<p>判别器的选择：全卷积网络，leaky ReLU 0.2，不使用batch-normalization层因为使用了小batchsize</p>
<p>语义分割网络：DeepLab-v2 不用multi-scale混合策略，</p>
<h2 id="代码">代码</h2>
<p>训练领域分类器使用的loss是BCE Loss，尽管论文中训练分类器的时候写的是softmax loss。 在对抗训练的特征提取器训练（以生成领域不变的特征）过程中，只使用了目标域的数据，将它分类成源域数据，和公式(4)一致。 流程是一轮对抗训练完成之后更新一次参数。</p>
<p>文章中没有说训练时采用的图像大小。 测试代码使用的是1024x512，但是在1080ti上使用1024 x 512显存会超出。</p>
<p>训练了512x256的模型，测试时使用512x256比1024x512的效果要好。</p>
<h1 id="conditional-generative-adversarial-network-for-structured-domain-adaptation">Conditional Generative Adversarial Network for Structured Domain Adaptation</h1>
<p><a href="#fn27" class="footnoteRef" id="fnref27"><sup>27</sup></a> CVPR2018 将GAN嵌入到FCN中实现域适应。具体地，学习一个条件生成器将生成图像的特征迁移到真实图像，判别器来判断是否是真实图像。对于每一个训练batch，条件生成器和判别器彼此竞争以生成更加真实的特征，然后FCN的参数更新来适应GAN的变化。</p>
<p>有文献指出对于结构化的预测学习一个决策函数，标签空间是指数级别的。</p>
<p>与Learning from synthetic data相比，首先，Learning from synthetic data还是基于共享特征空间的假设，特征提取器和GAN的生成器都是共享的，而这里源域图像额外经过了一个残差连接的结构，以实现虚拟特征到真实特征之间的转换。其次，Learning from synthetic中判别的对象在像素空间，而这里判别的对象在特征空间；最后，learning form synthetic中有虚拟到真实和真实到虚拟两种变换，判别器有领域内真假的判别和跨领域真实和生成之间的判别，这里只有虚拟到真实的变换，判别器只需要进行跨领域的判别。</p>
<p>模型的网络结构如下图所示： <img src="/2018/09/05/DA-in-semantic-segmentation/conditional-GAN.JPG" title="基于条件GAN的分割域适应模型"></p>
<p>文章在第一节末尾的写法和PixelDA类似： &gt; Our unsupervised pixel-level domain adaptation method (PixelDA) offers a number of advantages over existing approaches:</p>
<blockquote>
<p>Our unsupervised domain adaptation method offers the following advantages over existing approaches:</p>
</blockquote>
<h2 id="主要贡献">主要贡献</h2>
<p>如何才能不依赖于源域和目标域共享一个特征空间上的预测函数？作者提出学习源域和目标域的特征空间的残差。在学习源域到目标之间的变换关系的同时，保留源域的语义空间结构。通过GAN来实现这样的变换。</p>
<ul>
<li>不假设共享的特征空间：思想来源于Residual Transfer network。实验结果表明，这种残差学习的思想是解决结构化域适应问题的关键</li>
<li>统一的网络结构：no more discrimination和curriculum更多的是依赖于像素的分布和标签的统计数据来进行域适应，本文通过条件生成器来变换特征，这样所有的结构都集中在一个网络中，可以端到端的训练。</li>
<li>数据增强：通过依赖于源域图像和随机噪声的生成器，可以创造更多的虚拟样本</li>
</ul>
<h2 id="训练">训练</h2>
<p>值得注意的是这里的判别器使用的是全连接网络，最终只输出一个概率值，而不是像其他方法一样在特征空间的多个位置上进行分类。</p>
<p>在分割的分类网络的训练中，使用了adapted和non-adapted的特征图进行训练。单纯在adpted之后的网络上训练不稳定，需要多次初始化和学习率设置。 Notice that we train T with both adapted and non-adapted source feature maps. Training T solely on adapted feature maps leads to similar performance, but requires many runs with different initializations and learning rates due to the instability of the GAN. Indeed, without train- ing on source as well, the model is free to shift class assignments (e.g. class 1 becomes 2, class 2 becomes 3 etc.), meanwhile the objective function is still optimized. Similiar to PixelDA, training classifier T on both source and adapted images avoids this shift and greatly stabilizes training. ## 相关工作 FCN in the wild Curriculum domain adaptation (CDA) No more discrimination Cross city adaptation (CCA) ROAD</p>
<p>前面三个更多的是依赖于像素的分布和标签的统计数据来进行域适应，本文通过条件生成器来变换特征，这样所有的结构都集中在一个网络中，可以端到端的训练。</p>
<h2 id="指标-5">指标</h2>
<p>基础网络是FCN-8s，VGG19，能达到这个效果还是很好的 GTA5→Cityscapes NoAdapt 21.1 adapt 44.5 SYNTHIA→Cityscapes noadapt 17.4 adapt 41.2</p>
<p>关于生成器部分的对比实验，为什么不使用生成器，单纯的对抗训练效果比不迁移还差呢？</p>
<h2 id="训练-1">训练</h2>
<p>图像缩放到<span class="math inline">\(480\times960\)</span>，Conv1的特征图大小是<span class="math inline">\(64\times339\times579\)</span>，所以随机噪声的大小是<span class="math inline">\(1\times\339\times579\)</span>，组成<span class="math inline">\(65\times\339\times579\)</span>，然后经过<span class="math inline">\(3\times3\)</span>的卷积，恢复成64通道。后面卷积都是64通道的。</p>
<h1 id="class-balanced-self-training">Class-Balanced Self-Training</h1>
<p><a href="#fn28" class="footnoteRef" id="fnref28"><sup>28</sup></a> ECCV2018 文章通过迭代的自学习流程来解决无监督的域适应问题，具体地，为目标样本生成伪标签，然后在用伪标签训练模型，如此不断迭代。在自学习的基础上，也提出了一个新的类别平衡自学习框架来克服伪标签生成过程中的逐渐变大的类别的主导地位，并且引入了空间先验来微调生成的标签。</p>
<p>不同类别之间的迁移难度是不一样的，原因可能是因为视觉差异，源域中类别分布不平衡，以及源域分布和目标分布不一样。</p>
<p>数据集：Cityscapes NTHU(Rome, Rio, Tokyo,Taipei) GTA5 Cityscapes SYNTHIA Cityscapes</p>
<h1 id="deep-activation-matching">deep activation matching</h1>
<p><a href="#fn29" class="footnoteRef" id="fnref29"><sup>29</sup></a> ECCV2018 通过对齐中间层的输出的分布来解决域适应问题。一方面，中间层的输入可以为网络的训练引入更多的约束。另一方面，匹配之后的激活之可以为下一层提供相似的输入，从而减轻covarite shift。使用了GAN来对齐激活值的分布。</p>
<p>网络结果如下图所示： <img src="/2018/09/05/DA-in-semantic-segmentation/deep-activation-matching-framework.JPG" title="deep-activate-matching网络结果"> 文章中引入了以下约束来解决目标域没有标注信息的问题 - 标签分布：假设源域和目标域上的标签类别分布是相似的。 - 激活值分布：在中间层的输出上进行分布的匹配。 - 权重漂移：域适应问题通常假设源域的表示会携带目标域的信息，仅仅需要微调来提升目标域的表现。上述的损失函数不能捕捉这一逐渐的变化。通过引入源域网络和目标域网络的正则化项，保证适应的过程中权重变化不要太大。实验中使用的是2范数</p>
<p>标签分布和激活值分布的匹配的损失函数是JS熵，使用了对抗训练的模式。</p>
<p>数据集： GTA5 Cityscapes ERFNet 31.3 FCN8s-VGG16 32.6 Dilated Residual Network 40.2 SYNTHIA Cityscapes</p>
<h2 id="训练-2">训练</h2>
<p>图像1024x512 在Cityscapes的测试集（1525）上的结果 在语义分割任务里使用了Least Square GAN来稳定训练</p>
<h1 id="effective-use-of-synthetic-data">Effective Use of Synthetic Data</h1>
<p><a href="#fn30" class="footnoteRef" id="fnref30"><sup>30</sup></a> ECCV2018 在训练过程中不需要看到任何的真实图片。文章的motivation是前景和背景类别的领域偏移的方式不同，因此需要分别对待。具体地，前景类别的形状通常是一样的，而纹理有差异，使用基于检测的方式来处理，而背景通常更加真实。</p>
<p>作者比较了DeepLab模型和Mask R-CNN模型在生成数据上的前景物体的mIOU，Mask R-CNN模型的表现更好。因此，使用语义分割模型来处理背景，基于检测的方法来处理前景物体。</p>
<p>数据集：GTAV，SYNTHIA，VIPER(proposed)，Cityscapes，CamVid</p>
<h1 id="cycada">Cycada</h1>
<p><a href="#fn31" class="footnoteRef" id="fnref31"><sup>31</sup></a> ICML2018 引入了循环一致性的对抗式域适应，在像素层面和特征层面进行适配，不需要成对的图像。 <a href="https://github.com/jhoffman/cycada_release" target="_blank" rel="external">代码，pytorch</a> ## motivation Cycada通过循环一致性图像到图像的转换将特征级别（DANN,ADDA）和图像级别(Unsupervised cross-domain image generation ICLR2017,Unsupervised pixel-level domain adaptation with generative adversarial networks，CVPR2017)的对抗式域适应方法结合起来。使用重建损失（循环一致性损失）来保留局部的结构信息，使用语义损失来保持语义的一致性。</p>
<p>这篇是不是将第一次将循环一致性约束引入域适应的？</p>
<p>循环一致性的论文如下： Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In International Conference on Computer Vision (ICCV), 2017.</p>
<p>网络结构如下图所示，其中对抗判别器<span class="math inline">\(D_T\)</span>是用来保证转换后的源域图像和目标域图像的风格一致性。为了保证内容的一致性，使用了循环一致性约束（如果只是对源域图像和转换后的源域图像进行内容一致性约束呢？）。为了实现源域图像和转换后的源域图像之间的语义一致性，使用了一个在源域上训练好的语义分割模型，参数固定，损失函数仍然是语义分割任务的损失，转换后的图像作为输出，未转换的图像经过语义分割模型生成的标签作为“真实标签”。与风格转换和像素适应中的内容损失类似。</p>

<p>图中的Reconstructed Source Image图像是不是有问题，内容应该和源域的图像是一致的才对啊。 将上图与Learning From Synthetic Data中的结构对比，</p>
<p>Similar to the PixelDA method [1], one concurrent work, CyCADA [12] uses the CycleGAN [42] and transfers source domain images to the target domain with pixel alignment, thus generating extra training data combined with feature space adversarial learning [13]. ----<a href="#fn32" class="footnoteRef" id="fnref32"><sup>32</sup></a></p>
<h2 id="相关工作-1">相关工作</h2>
<p>The representation is optimized using the standard minimax objective (Ganin &amp; Lempitsky, 2015), the symmetric confusion objective (Tzeng et al., 2015), or the inverted label objective (Tzeng et al., 2017). Each of these objectives is related to the literature on generative adversarial networks (Goodfellow et al., 2014) and follow-up work for improved training procedures for these networks (Salimans et al., 2016b; Arjovsky et al., 2017).</p>
<p>特征空间的对齐-&gt;对抗式学习(discriminative representation space)-&gt;生成式模型(pixel-space)</p>
<p>图像风格转换，其中怎么利用对抗式方法？ 源域图像，转换后的源域图像，和目标域的图像，三者之间的loss应该怎么用。源域图像和转换后的源域图像之间的内容应该是相似的，转换后的源域图像和目标域的图像之间的风格是相似的。</p>
<p>非成对的图像转换</p>
<h2 id="指标-6">指标</h2>
<p>GAT5 -&gt; Cityscapes FCN-8s pixel+feat 35.4</p>
<h1 id="maximum-classifier-discrepancy">Maximum classifier discrepancy</h1>
<p><a href="#fn33" class="footnoteRef" id="fnref33"><sup>33</sup></a> CVPR2018，oral <a href="https://github.com/mil-tokyo/MCD_DA" target="_blank" rel="external">代码，pytorch</a> 许多对抗式的方法训练一个领域分类网络来区分特征是来自于源域还是目标域，或者训练一个生成器来模仿判别器。这些方法存在两个问题，首先领域分类器仅仅试图区分特征来自于哪里，并没有考虑任务相关的类别之间的决策边界，使得生成器产生的样本生成类别边界附近的有歧义的样本。其次，这些方法期望完全的匹配不同领域之间的特征分布，但领域的属性不同，导致这一目标很困难。</p>
<p>为了解决这些问题，引入了一种新方法，利用任务相关的决策边界来对齐源域和目标域之间的分布。我们提出最大化两个分类器输出的差异来检测远离源域支持面的目标域中的样本（有使分类器学到更紧致的界的效果？）。特征生成器用来学习生成目标域的特征来最小化分类器之间的差异（使得目标域的特征趋近于源域？）。</p>
<h2 id="指标-7">指标</h2>
<p>GTA5-&gt;Cityscapes VGG-16 28.8</p>
<h1 id="dual-channel-wise-alignment-networks">Dual Channel-wise Alignment Networks</h1>
<p><a href="#fn34" class="footnoteRef" id="fnref34"><sup>34</sup></a> ECCV2018 Dual Channel-wise Alignment Networks是一个在像素级别和特征级别减少领域偏移的网络。为了利用CNN特征的每一个通道的统计数据，在基于通道的特征上对齐，在图像生成器和分割网络上都保留了空间结构和语义信息。具体地，给定了一副源域图像和无标注的目标域图像，生成器在线地合成外观类似于目标域的图像，分割网络进一步地微调高级特征以生成最后的语义图。和最近的依靠对抗训练的网络不同，本文的框架更轻量且容易训练。</p>
<p>网络结构如下图所示： </p>
<p>CNN特征图的每一个通道的均值和方差可以看做体现了图像的风格信息，因此，可以通过简单的实例归一化步骤来实现特征图的逐通道的对齐，从而实现快速的风格转换。实际使用了adaptive instance normalization<a href="#fn35" class="footnoteRef" id="fnref35"><sup>35</sup></a>来匹配均值和方差，将源域的多个特征图转换到和和目标域的特征图的分布一致。之后再对特征解码生成新的图像，新的图像和原始的源域图像之间进行内容（欧式距离）和风格（Gram矩阵）的约束。</p>
<p>同样地，在语义分割网络中也使用了逐通道的对齐。具体地，将语义分割模型分成编码器和解码器，在编码器的输出上应用一次逐通道的对齐，然后送入解码器。</p>
<h2 id="指标-8">指标</h2>
<p>GTA5 FCN8s-VGG16 36.2/27.8 FCN8s-ResNet101 38.5/29.8 PSPNet 41.7/33.3</p>
<h2 id="实验结果分析">实验结果分析</h2>
<h1 id="fcan">FCAN</h1>
<p>CVRP2018 In this paper, we facilitate this issue from the perspectives of both visual appearance-level and representation-level do- main adaptation. The former adapts source-domain images to appear as if drawn from the “style” in the target domain and the latter attempts to learn domain-invariant representations.</p>
<p>在对抗损失的前面增加了一个风格迁移的网络，对抗的分类器中引入了ASPP</p>
<p>整体的网络结构如下图 <img src="/2018/09/05/DA-in-semantic-segmentation/FCAN-framework.JPG" title="FCAN网络结构"></p>
<p>风格迁移网络AAN（Appearance Adaptation Network）的结构 <img src="/2018/09/05/DA-in-semantic-segmentation/FCAN-AAN-framework.JPG" title="AAN的网络结构"></p>
<p>与Learning From Synthetic Data，这里虽然也有fake source,fake target，但是只用用在了领域之间的对抗，而没有真假的对抗。</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>S. Hong, J. Oh, H. Lee, and B. Han. Learning transferrable knowledge for semantic segmentation with deep convolutional neural network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3204–3212, 2016.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>A. Kolesnikov and C. H. Lampert. Seed, expand and constrain: Three principles for weakly-supervised image segmentation. In European Conference on Computer Vision, pages 695–711. Springer, 2016.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>W. Shimoda and K. Yanai. Distinct class-specific saliency maps for weakly supervised semantic segmentation. In European Conference on Computer Vision, pages 218–234. Springer, 2016.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>J. Hoffman, D. Wang, F. Yu, and T. Darrell. Fcns in the wild: Pixel-level adversarial and constraint-based adaptation. arXiv preprint arXiv:1612.02649, 2016.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Tsai Y H, Hung W C, Schulter S, et al. Learning to adapt structured output space for semantic segmentation[J]. arXiv preprint arXiv:1802.10349, 2018. CVPR2018<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Y. Zhang, P. David, and B. Gong. Curriculum domain adaptation for semantic segmentation of urban scenes. In The IEEE International Conference on Computer Vision (ICCV), volume 2, page 6, 2017.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>Y. Chen, W. Li, and L. Van Gool. Road: Reality oriented adaptation for semantic segmentation of urban scenes. In CVPR, 2018.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Sankaranarayanan S, Balaji Y, Jain A, et al. Learning From Synthetic Data: Addressing Domain Shift for Semantic Segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 3752-3761.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>Hoffman J, Tzeng E, Park T, et al. Cycada: Cycle-consistent adversarial domain adaptation[J]. arXiv preprint arXiv:1711.03213, 2017.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>Saito K, Watanabe K, Ushiku Y, et al. Maximum classifier discrepancy for unsupervised domain adaptation[J]. arXiv preprint arXiv:1712.02560, 2017, 3.<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>DCAN: Dual Channel-wise Alignment Networks for Unsupervised Scene Adaptation.<a href="#fnref11">↩</a></p></li>
<li id="fn12"><p>Y.-H. Chen, W.-Y. Chen, Y.-T. Chen, B.-C. Tsai, Y.-C. F. Wang, and M. Sun. No more discrimination: Cross city adaptation of road scene segmenters. In ICCV, 2017.<a href="#fnref12">↩</a></p></li>
<li id="fn13"><p>Y. Chen, W. Li, and L. Van Gool. Road: Reality oriented adaptation for semantic segmentation of urban scenes. In CVPR, 2018.<a href="#fnref13">↩</a></p></li>
<li id="fn14"><p>W. Shimoda and K. Yanai. Distinct class-specific saliency maps for weakly supervised semantic segmentation. In European Conference on Computer Vision, pages 218–234. Springer, 2016.<a href="#fnref14">↩</a></p></li>
<li id="fn15"><p>J. Hoffman, D. Wang, F. Yu, and T. Darrell. Fcns in the wild: Pixel-level adversarial and constraint-based adaptation. arXiv preprint arXiv:1612.02649, 2016.<a href="#fnref15">↩</a></p></li>
<li id="fn16"><p>Y.-H. Chen, W.-Y. Chen, Y.-T. Chen, B.-C. Tsai, Y.-C. F. Wang, and M. Sun. No more discrimination: Cross city adaptation of road scene segmenters. In ICCV, 2017.<a href="#fnref16">↩</a></p></li>
<li id="fn17"><p>Sankaranarayanan S, Balaji Y, Jain A, et al. Learning From Synthetic Data: Addressing Domain Shift for Semantic Segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 3752-3761.<a href="#fnref17">↩</a></p></li>
<li id="fn18"><p>Tsai Y H, Hung W C, Schulter S, et al. Learning to adapt structured output space for semantic segmentation[J]. arXiv preprint arXiv:1802.10349, 2018. CVPR2018<a href="#fnref18">↩</a></p></li>
<li id="fn19"><p>Saito K, Watanabe K, Ushiku Y, et al. Maximum classifier discrepancy for unsupervised domain adaptation[J]. arXiv preprint arXiv:1712.02560, 2017, 3.<a href="#fnref19">↩</a></p></li>
<li id="fn20"><p>Sankaranarayanan S, Balaji Y, Jain A, et al. Learning From Synthetic Data: Addressing Domain Shift for Semantic Segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 3752-3761.<a href="#fnref20">↩</a></p></li>
<li id="fn21"><p>W. Shimoda and K. Yanai. Distinct class-specific saliency maps for weakly supervised semantic segmentation. In European Conference on Computer Vision, pages 218–234. Springer, 2016.<a href="#fnref21">↩</a></p></li>
<li id="fn22"><p>J. Hoffman, D. Wang, F. Yu, and T. Darrell. Fcns in the wild: Pixel-level adversarial and constraint-based adaptation. arXiv preprint arXiv:1612.02649, 2016.<a href="#fnref22">↩</a></p></li>
<li id="fn23"><p>Tsai Y H, Hung W C, Schulter S, et al. Learning to adapt structured output space for semantic segmentation[J]. arXiv preprint arXiv:1802.10349, 2018. CVPR2018<a href="#fnref23">↩</a></p></li>
<li id="fn24"><p>W. Shimoda and K. Yanai. Distinct class-specific saliency maps for weakly supervised semantic segmentation. In European Conference on Computer Vision, pages 218–234. Springer, 2016.<a href="#fnref24">↩</a></p></li>
<li id="fn25"><p>Y.-H. Chen, W.-Y. Chen, Y.-T. Chen, B.-C. Tsai, Y.-C. F. Wang, and M. Sun. No more discrimination: Cross city adaptation of road scene segmenters. In ICCV, 2017.<a href="#fnref25">↩</a></p></li>
<li id="fn26"><p>Hoffman J, Tzeng E, Park T, et al. Cycada: Cycle-consistent adversarial domain adaptation[J]. arXiv preprint arXiv:1711.03213, 2017.<a href="#fnref26">↩</a></p></li>
<li id="fn27"><p>Hong W, Wang Z, Yang M, et al. Conditional Generative Adversarial Network for Structured Domain Adaptation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 1335-1344.<a href="#fnref27">↩</a></p></li>
<li id="fn28"><p>Zou Y, Yu Z, Vijaya Kumar B V K, et al. Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 289-305.<a href="#fnref28">↩</a></p></li>
<li id="fn29"><p>Huang H, Huang Q, Kraehenbuehl P. Domain transfer through deep activation matching[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 590-605.<a href="#fnref29">↩</a></p></li>
<li id="fn30"><p>Saleh F S, Aliakbarian M S, Salzmann M, et al. Effective Use of Synthetic Data for Urban Scene Semantic Segmentation[J]. arXiv preprint arXiv:1807.06132, 2018.<a href="#fnref30">↩</a></p></li>
<li id="fn31"><p>Hoffman J, Tzeng E, Park T, et al. Cycada: Cycle-consistent adversarial domain adaptation[J]. arXiv preprint arXiv:1711.03213, 2017.<a href="#fnref31">↩</a></p></li>
<li id="fn32"><p>Tsai Y H, Hung W C, Schulter S, et al. Learning to adapt structured output space for semantic segmentation[J]. arXiv preprint arXiv:1802.10349, 2018. CVPR2018<a href="#fnref32">↩</a></p></li>
<li id="fn33"><p>Saito K, Watanabe K, Ushiku Y, et al. Maximum classifier discrepancy for unsupervised domain adaptation[J]. arXiv preprint arXiv:1712.02560, 2017, 3.<a href="#fnref33">↩</a></p></li>
<li id="fn34"><p>DCAN: Dual Channel-wise Alignment Networks for Unsupervised Scene Adaptation.<a href="#fnref34">↩</a></p></li>
<li id="fn35"><p>Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance normalization. In: ICCV. (2017)<a href="#fnref35">↩</a></p></li>
</ol>
</div>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/03/encoding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/03/encoding/" itemprop="url">
                  字符和编码
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Post created: 2018-09-03 22:00:23" itemprop="dateCreated datePublished" datetime="2018-09-03T22:00:23+08:00">2018-09-03</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Updated at: 2018-09-04 14:58:57" itemprop="dateModified" datetime="2018-09-04T14:58:57+08:00">2018-09-04</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="文件编码">文件编码</h1>
<p>参考<a href="https://blog.csdn.net/gatieme/article/details/55045883" target="_blank" rel="external">这篇博客</a> 里面包含了linux下查看文件编码的方法</p>
<p>pycharm里也可以修改文本文件的编码， 位置在右下角 提供两个选项，reload convert</p>
<p>reload不改变文件时间的二进制值，只是修改读取方式，适用于出现了乱码需要寻找合适编码的情况</p>
<p>convert是使用不同的编码保存文件，会修改文件保存出来的二进制值，以及文件大小。</p>
<h1 id="字符和编码">字符和编码</h1>
<p><a href="http://www.ruanyifeng.com/blog/2007/10/ascii_unicode_and_utf-8.html" target="_blank" rel="external">这篇博客</a>探讨不同概念的来源，没有涉及python2中的str和unicode，可以看下面的博客来理解</p>
<p>关于python2中的str和unicode看<a href="https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386819196283586a37629844456ca7e5a7faa9b94ee8000" target="_blank" rel="external">这篇博客</a></p>
<img src="/2018/09/03/encoding/encoding-1.JPG" title="encoding和py2的字符，unicode">
<img src="/2018/09/03/encoding/encoding-2.JPG" title="py2中str和unicode的转换">
<h1 id="py文件">py文件</h1>
<p>在py2的文件一开始，通常会写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br></pre></td></tr></table></figure>
<p>这句话的作用是为了告诉Python解释器，按照UTF-8编码读取源代码，否则，你在源代码中写的中文输出可能会有乱码。 这句代码并不能保证.py文件自身的编码格式</p>
<p>需要设置.py文件自身的编码格式是UTF-8，而不是其他如GBK等，pycharm中如果开头添加了这句话，那么右下角的文件编码格式会自动锁定UTF-8，还是很贴心的。</p>
<h1 id="py2-和-py3的区别">py2 和 py3的区别</h1>
<p>fluent python的作者写的<a href="https://speakerdeck.com/ramalho/unicode-solutions-in-python-2-and-python-3?slide=22" target="_blank" rel="external">unicode solution</a></p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/15/weak-pairwise-constraints/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/15/weak-pairwise-constraints/" itemprop="url">
                  Adapting Deep Visuomotor Representations with Weak Pairwise Constraints
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Post created: 2018-08-15 16:22:04 / Updated at: 16:25:18" itemprop="dateCreated datePublished" datetime="2018-08-15T16:22:04+08:00">2018-08-15</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="思路">思路</h1>
<p>针对的是虚拟数据到真实数据的无监督域适应的问题，并且希望利用虚拟数据和真实数据之间的成对关系，于是在训练中基于特征人为构造了成对数据，并且和网络训练迭代进行。</p>
<h1 id="实现">实现</h1>
<p>网络结构如下图所示： <img src="/2018/08/15/weak-pairwise-constraints/network.JPG" title="proposed network"></p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/15/simultanueous-deep-transfer-across-domain-and-task/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/15/simultanueous-deep-transfer-across-domain-and-task/" itemprop="url">
                  Simultaneous deep transfer across domains and tasks
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Post created: 2018-08-15 13:29:54 / Updated at: 16:09:47" itemprop="dateCreated datePublished" datetime="2018-08-15T13:29:54+08:00">2018-08-15</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="思路">思路</h1>
<p>本文针对的情况是目标域存在少量稀疏的带标签样本。通过domain confusion loss来获取领域一致的特征表示，同时由源域样本得到的soft label，建模类别之间的关系，指导目标域样本的分类，效果比hard label要好。</p>
<h1 id="方法">方法</h1>
<p>网络结构如下图所示： <img src="/2018/08/15/simultanueous-deep-transfer-across-domain-and-task/network.JPG" title="proposed network"></p>
<p>fcD层后面接的domain confusion loss和domain classifier loss是实现对抗学习的，目标和by backpropagation一样。</p>
<h2 id="aligning-domain-via-domain-confusion">Aligning domain via domain confusion</h2>
<p>首先要学习一个好的domain classifier，这样才能判别特征是不是真的有区分性，这个loss就是真实标签（图像属于哪个领域）和网络输出之间交叉熵损失。 <span class="math display">\[L_D(x_S,x_T,\theta_{repr}; \theta_D) = - \sum 1[y_D=d]logq_d\]</span></p>
<p>另外，为了最大程度的使domain classifier混淆，构造均匀分布和网络输出之间的交叉熵损失。 <span class="math display">\[L_D(x_S,x_T,\theta_D; \theta_{repr}) = - \sum\frac{1}{D} logq_d\]</span></p>
<p>这两部分的优化方向对于特征提取网络来说是冲突的，因此是迭代训练的。</p>
<h2 id="aligning-source-and-target-classes-via-soft-labels">Aligning source and target classes via soft labels</h2>
<p>对于都在目标域中存在的部分稀疏的标签，也将他们送到domain classifier中进行分类，但是使用hard label使得网络局限在提供的少量样本和少量类别中，因此受到网络蒸馏<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a><a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>的启发，一个大的网络可以“蒸馏”到一个小的网络中，通过将小网络的学习目标替换成大网络输出的soft label.这样可以使网络兼顾更多的类别。</p>
<p>简单来说，就是将source domain中每一类的所有样本的softmax输出的概率向量取平均，作为该类的soft label，用于target domain的交叉熵中需要拟合的分布。这样的soft label可以反映类别之间的相似关系。</p>
<p>在实际的实验中，一开始就在source domain上微调caffenet，然后生成soft label，后面就不动了（？）。</p>
<h1 id="代码">代码</h1>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>J. Ba and R. Caruana. Do deep nets really need to be deep? In Z. Ghahramani, M.Welling, C. Cortes, N. Lawrence, and K.Weinberger, editors, Advances in Neural Information Pro- cessing Systems 27, pages 2654–2662. Curran Associates, Inc., 2014.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representa- tion Learning Workshop, 2014.<a href="#fnref2">↩</a></p></li>
</ol>
</div>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/15/adversarial-discriminative-domain-adaption/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/15/adversarial-discriminative-domain-adaption/" itemprop="url">
                  Adversarial Discriminative Domain Adaption
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Post created: 2018-08-15 10:50:05 / Updated at: 10:50:21" itemprop="dateCreated datePublished" datetime="2018-08-15T10:50:05+08:00">2018-08-15</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="abstract">Abstract</h1>
<p>之前的生成式方法展示了很好的视觉效果，但是在判别式任务上不是最优的，并且局限于较小的偏移；之前的判别式方法可以处理较大的偏移，引入了附加的权重，并且没有利用gan loss. 本文第一次为对抗式域适应提供了一个通用的框架，可以将最近的方法看做是该框架的一个特例。 # Generalized adverarial adaptation</p>
<h2 id="source-and-target-mappings-in">Source and target mappings In</h2>
<p>Coupled generative adversarial networks 在MNIST的无监督学习学习任务上取得了state-of-the-art 由于 ## Adversarial losses</p>
<h1 id="adversarial-discriminative-domain-adaptation">Adversarial discriminative domain adaptation</h1>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/13/unsupervised-image-to-image-translation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/13/unsupervised-image-to-image-translation/" itemprop="url">
                  Unsupervised Image-to-Image Translation Networks(UNIT)
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Post created: 2018-08-13 09:04:25 / Updated at: 22:33:15" itemprop="dateCreated datePublished" datetime="2018-08-13T09:04:25+08:00">2018-08-13</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="思路">思路</h1>
<p>本文同样关注unsupvised设置，即只知道两个领域图像各自的边缘分布<span class="math inline">\(P_{x1}\)</span>和<span class="math inline">\(P_{x2}\)</span>，而不知道联合分布<span class="math inline">\(P_{x1,x2}\)</span>,和作者的上一篇Coupled GAN<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>一样. 上一篇论文的介绍看这里<a href="./coupled-gan.md">Coupled GAN</a>.</p>
<p>作者同样遵循了Coupled GAN中的假设：即对于不同领域的成对的图像<span class="math inline">\(x_1\)</span>和<span class="math inline">\(x_2\)</span>，存在一个共享的隐空间编码<span class="math inline">\(z\)</span>，既可以由图像计算编码<span class="math inline">\(z\)</span>，也可以从<span class="math inline">\(z\)</span>恢复两幅图像。Coupled GAN中两个生成器使用同一个随机变量作为输入，从而生成各自领域的图像。作者将生成器作为解码器，随机变量是隐空间的编码，在生成器前加入解码器。模型的结构如下： <img src="/2018/08/13/unsupervised-image-to-image-translation/UNIT-network-arch.JPG" title="network architecture of UNIT"></p>
<p>正因为有了同一个隐空间的表示，所以<span class="math inline">\(G_1(E_2(G_2(E_1(x_1))))\)</span>的输出就是<span class="math inline">\(x_1\)</span>，同理<span class="math inline">\(x_2\)</span>也可以实现cycle consistency，类似于<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>。但是由cycle consistency并不能推出这里的结构。</p>
<h1 id="代码">代码</h1>
<p>代码在<a href="https://github.com/mingyuliutw/unit" target="_blank" rel="external">这里</a></p>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>M.-Y. Liu and O. Tuzel. Coupled generative adversarial networks. Advances in Neural Information Processing Systems, 2016.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>T. Kim, M. Cha, H. Kim, J. Lee, and J. Kim. Learning to discover cross-domain relations with generative adversarial networks. International Conference on Machine Learning, 2017.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. International Conference on Computer Vision, 2017.<a href="#fnref3">↩</a></p></li>
</ol>
</div>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/12/coupled-gan/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/12/coupled-gan/" itemprop="url">
                  Coupled GAN
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Post created: 2018-08-12 16:26:18" itemprop="dateCreated datePublished" datetime="2018-08-12T16:26:18+08:00">2018-08-12</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Updated at: 2018-08-14 22:12:49" itemprop="dateModified" datetime="2018-08-14T22:12:49+08:00">2018-08-14</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="思路">思路</h1>
<p>提出可以学习多领域图像的联合分布的coupled GAN，不需要不同领域之间图像的对应关系。这是通过强制权重共享来限制网络容量，并且偏向于不同边缘分布的乘积来构造一个联合分布。</p>
<h1 id="方法">方法</h1>
<h2 id="网络结构">网络结构</h2>
<img src="/2018/08/12/coupled-gan/coupled-gan.JPG" title="network of coupled gan">
<p>采用了两个并列的GAN,各自的输入是不同领域的边缘分布采样的结果。GAN的生成器将<strong>同一个</strong>随机变量（高层语义信息）映射到不同的图像空间，并且通过权重共享的方式来约束不同领域的图像具有相同的高层信息，以及不同的底层实现方式。</p>
<p>生成器的输入只是随机变量，没有不同领域的图像（不同于conditional GAN），判别器比较的是各自领域的真实图像和生成图像。</p>
<p>结合下面的公式来理解： <img src="/2018/08/12/coupled-gan/coupled-gan-learning-formula.JPG" title="learning formula of coupled gan"></p>
<h2 id="生成器">生成器</h2>
<p>生成模型的前面几层用来解码高层的语义信息（输入的随机变量），而判别模型的后面几层用来提取高层语义信息（用于判断是否是真实图像）。</p>
<p>不同领域的对应图像共享比较抽象的语义信息，所以这里将生成器的前几层和判别器的后面几层的权重共享。</p>
<h2 id="判别器">判别器</h2>
<p>判别器的后面几层的权重也共享了，<strong>但是对学习联合分布是不重要的</strong>，可以帮助减少模型的参数量。</p>
<h2 id="重点">重点</h2>
<p>本文的主要贡献在于验证了只对<strong>边缘分布</strong><span class="math inline">\(P_{x1}\)</span>和<span class="math inline">\(P_{x2}\)</span>采样（每次两个领域的真实图像大概率是不对应的），学习到不同图像领域之间的配对关系（也就是<strong>联合分布</strong><span class="math inline">\(P_{x1,x2}\)</span>）。</p>
<p>本文的结构下也完成将一个领域的特定图像转换到另一个领域的任务。</p>
<p>自编码器学习<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>通过鼓励生成的<strong>图像对</strong>和目标的<strong>图像对</strong>之间的一致性来最小化loss（因此需要不同领域图像的对应关系），与之不同的是，本文采用的对抗学习仅仅是鼓励生成的<strong>图像对</strong>中的图像和各自领域的图像更像。（这么来看，还是很神奇的）。</p>
<p>解释是生成模型必须更加有效地利用模型参数来欺骗判别器，由于参数的共享，<strong>最有效的方式就是生成对应的图像</strong>。</p>
<p>CoGAN需要不同领域之间有共享的高层表示，如果没有的话，就会失败。（实验中有例子？）</p>
<h1 id="实验">实验</h1>
<p>评价指标：the pixel agreement ratio—the number of corresponding pixels that have the same value in the two images divided by the total image size. 在此之前还有没有类似的工作（不需要成对样本的监督），还是相当早的哈</p>
<p>MNIST上的任务： 1. 图像和边缘的对应 2. 图像和负的图像 3. 图像和90度旋转之后的图像</p>
<h2 id="wight-sharing">wight sharing</h2>
<p>生成模型的共享层越多，渲染的图像对就越像是从联合分布获取的真实样本。</p>
<h2 id="和条件gan的比较">和条件GAN的比较</h2>
<p>对比是这样的，条件GAN之前介绍过了，就是在普通GAN的生成器和判别器的输入中加入额外的信息。这里首先构建一个普通的GAN，其生成模型和判别模型和CoGAN中的一样。然后在生成模型和判别模型的输入中加入一个布尔值，用来指示图像来源于哪个领域。如果条件GAN也可以从边缘分布学习到联合分布的话，那么生成模型在同一个随机变量下，配合额外的布尔值，可以生成同一个数字（MNSIT上），但是风格不同。但实验表明，条件GAN并不能生成同样的数字。（可不可以加入其它限制，使得条件GAN也可以这样的任务？）</p>
<h2 id="人脸数据集">人脸数据集</h2>
<p>在CelebFaces Attributes dataset<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>上做了实验，这一数据集中每一张脸都有不同的属性，如金发，微笑，眼镜等。在训练的时候，将具有某项属性的图像作为Domain1，不具有这项属性的图像作为Domain2. 尽管这样的设置导致Domain1的图像数量少于Domain2，但是并没有影响训练。</p>
<p>实验结果显示同一个随机变量可以生成对应的具有某项属性和不具有这项属性的人脸图像，同时，随着随机变量的游走，两个领域的人脸变化会保持一致。</p>
<h2 id="应用">应用</h2>
<h3 id="无监督域适应">无监督域适应</h3>
<p>无监督域适应的问题是将一个在源域适用的分类器经过调整使之在目标域上也适用。</p>
<p>实验是从MNSIT到USPS，实验设置和<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a><a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>一样，从MNSIT中采样2000张图像，从USPS中，采样1800张图像，将USPS的分辨率缩放到和MNIST一样。</p>
<p>CoGAN用来生成图像，为了对<span class="math inline">\(D_1\)</span>的图像进行分类，在<span class="math inline">\(D_1\)</span>的判别器的最后一层里添加softmax层进行分类。CoGAN本身的训练和MNIST上的分类任务同时进行。注意到MNIST上的判别器和USPS上的判别器最后几层是共享的，USPS的判别器在分类USPS的数据时可以直接用。</p>
<p>在分类的时候，直接用真实图像作为判别器的输入，没有生成器的事，生成器只在训练的时候完成共享隐空间的设置。如果在判别器的后面几层不共享，不知道对于这个任务会不会有影响？如果只是单纯的用判别器部分来训练呢，可以完成UDA的任务吗？</p>
<p>对比的方法有<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a><a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a><a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a><a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p>
<h3 id="跨域图像转换">跨域图像转换</h3>
<p>从概率分布的角度看，跨域图像转换的任务是给定Domain1中的一副图像<span class="math inline">\(x_1\)</span>，找到在Domain2中的对应的图像，也最大化联合概率分布<span class="math inline">\(P_{x1,x2}\)</span>.</p>
<p>当然啦，受制于模型本身的结构，不能直接完成图像的跨域转换，做法感觉是没有办法的办法。</p>
<p>首先是定义一个同领域图像相似度的损失函数，对于Domain1的图像<span class="math inline">\(x_1\)</span>，寻找最优的输入生成器<span class="math inline">\(G_1\)</span>随机变量<span class="math inline">\(z^*\)</span>，之后<span class="math inline">\(x_2=g_2(x^*)\)</span>就可以获得Domain2的对应图像 <span class="math display">\[z^* = argmin_z L(g_1(z),x_1)\]</span> 文章中的损失函数使用欧式距离，并用L-BFGS优化算法（还不了解）</p>
<p>实验结果发现，之后当输入图像可以被生成器<span class="math inline">\(g_1\)</span>生成时，转换效果才好，如果不能，则会生成模糊的图像。</p>
<h1 id="实现">实现</h1>
<p>代码在这里 https://github.com/mingyuliutw/cogan</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multimodal deep learning. In ICML, 2011.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Ziwei Liu, Ping Luo, XiaogangWang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, 2015.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Mingsheng Long, JianminWang, Guiguang Ding, Jiaguang Sun, and Philip Yu. Transfer feature learning with joint distribution adaptation. In ICCV, 2013.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Artem Rozantsev, Mathieu Salzmann, and Pascal Fua. Beyond sharing weights for deep domain adaptation. arXiv:1603.06432, 2016.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Basura Fernando, Tatiana Tommasi, and Tinne Tuytelaars. Joint cross-domain classification and subspace learning for unsupervised adaptation. Pattern Recognition Letters, 65:60–66, 2015.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv:1412.3474, 2014.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>Mingsheng Long, JianminWang, Guiguang Ding, Jiaguang Sun, and Philip Yu. Transfer feature learning with joint distribution adaptation. In ICCV, 2013.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Artem Rozantsev, Mathieu Salzmann, and Pascal Fua. Beyond sharing weights for deep domain adaptation. arXiv:1603.06432, 2016.<a href="#fnref8">↩</a></p></li>
</ol>
</div>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/12/image-to-image-translation-conditional-gan/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/12/image-to-image-translation-conditional-gan/" itemprop="url">
                  Image-to-Image Translation with Conditional Adversarial Networks
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Post created: 2018-08-12 14:43:09" itemprop="dateCreated datePublished" datetime="2018-08-12T14:43:09+08:00">2018-08-12</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Updated at: 2018-08-15 10:31:43" itemprop="dateModified" datetime="2018-08-15T10:31:43+08:00">2018-08-15</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="思想">思想</h1>
<p>利用条件GAN来解决图像到图像的学习问题，不仅学习图像到图像的映射，同时学习一个损失函数来训练这一映射，使得这一方法可以应用于不同的任务，例如由标签生成图像，由边缘图像重建物体，为物体上色等。</p>
<p>生成器采用U-Net，判别器采用卷积“PatchGAN”分类器，仅在patch大小的区域对图像结构进行约束。</p>
<h1 id="方法">方法</h1>
<h2 id="网络结构">网络结构</h2>
<img src="/2018/08/12/image-to-image-translation-conditional-gan/pixel2pixel-network.JPG" title="network of pixel2piexl">
<p>随机噪声没有画出来</p>
<p>判别器判别的是{real_edge, real_photo} 和 {real_edge, fake_photo}的真假，这也是本文训练需要成对数据的原因。</p>
<h2 id="目标函数">目标函数</h2>
<p>使用条件GAN</p>
<p>在<strong>生成器</strong>网络上施加一个传统的loss函数，这里使用了L1 loss， L1会比L2更加清晰一些。目的是使生成器的输出和target domain的真实样本更加相似，可以看做是一个<strong>reconstruction loss</strong>。有了成对的数据，单纯使用Reconstruction loss也是可以训练得到结果的。实验中比较了只使用L1 loss的情况。</p>
<p>如果没有了噪声项，网络同样也可以学出来一个映射，但是这样会产生决定性的结果，从而不能够匹配除了delta函数之外的其他函数。但是作者初始的实验中，发现加入随机噪声并没有用，生成器还是会忽略随机噪声，这和<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>的观察一样。因此，在最终的模型中，在生成器的不同层添加了dropout（训练和测试时都用）。尽管如此，在输出的图像中，还是只观察到了很小的随机性。</p>
<h2 id="生成网络">生成网络</h2>
<p>image到image的转换是高分辨率的输入图像到高分辨率的输入图像的过程，图像之间的外观不同，但是图像的内在结构是一样的。 使用U-Net结构，带有skip连接，可以包含更多的底层信息。在图像着色的应用中，输入和输出共享了边缘的位置。</p>
<h2 id="markovian-discriminatorpatchgan">Markovian discriminator(PatchGAN)</h2>
<p>L1已经足够保证图像外观在低频的相似性了。</p>
<p>在高频，重点关注图像局部的结构。因此，针对高频的相似性，仅仅在图像patch级别做判断。</p>
<p>实验证明，patch的大小可以取得比全图小很多，同时图像具有很高的质量。</p>
<p>理论分析： 这样的判别器有效地将图像建模成一个马尔科夫随机场，假设不同patch内的像素是独立的。这一观点在PatchGAN<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>中进行了探索，也是其他领域如纹理生成，图像风格迁移<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a><a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>等的常识。</p>
<h2 id="优化和推断">优化和推断</h2>
<p>在测试的时候比较特殊，也使用了dropout。</p>
<p>BN层的参数使用的是测试batch的数据，而不是训练集上累计得到的数据。</p>
<p>当 batch size设为1的时候，就变成了实例归一化<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>，这在图像生成任务上证明是有效的，实验中采用的batchsize在1到10之间。</p>
<h1 id="实验">实验</h1>
<h2 id="评价指标">评价指标</h2>
<p><strong>FCN-score</strong>: 利用在数据集上训练好的FCN-8s模型，对由标签生成的图像进行语义分割的任务，看语义分割的指标，生成的图像越真实，语义分割的各项指标也应该越高。（如果是由图像生成标签，那么应该可以直接计算指标，原文给的Table5是labels到photo的双向箭头，不知道是不是两个任务都考虑了？）</p>
<h2 id="color">color</h2>
<p>L1 loss生成的图像，颜色分布最窄。加入cGAN可以拓宽颜色分布。</p>
<h2 id="pixelgan-patchgan-imagegan">PixelGAN PatchGAN ImageGAN</h2>
<p>改变判别器的patch大小，会有不同的效果。</p>
<p>1x1的patch对于是使图像锐化没有效果，但是可以增大颜色的分布。</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error. ICLR, 2016. PatchGAN<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>C. Li and M. Wand. Precomputed real-time texture synthesis with markovian generative adversarial networks. ECCV, 2016.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Image style transfer using convolutional neural networks. CVPR 2016<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Combining markov random fields and convolutional neural networks for image synthesis. CVPR 2016<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normal- ization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016<a href="#fnref5">↩</a></p></li>
</ol>
</div>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">John Doe</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">14</span>
                    <span class="site-state-item-name">Artikel</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">11</span>
                    <span class="site-state-item-name">Tags</span>
                  
                </div>
              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  

  
</div>




  <div class="powered-by">Erstellt mit  <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> v3.4.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Muse</a> v6.3.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



  










  





  

  

  

  
  

  
  
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    
  


  
  

  

  

  

  

  

</body>
</html>
